{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aadarsh0709/gnnmodelpfas/blob/main/gnnidsmilesgraphconversion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch.geometric\n",
        "!pip install rdkit\n",
        "!pip install pubchempy"
      ],
      "metadata": {
        "id": "8-Kg7Nvot7YR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3343e5b4-d402-49c0-d18f-8273bbfb292c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch.geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch.geometric) (3.11.10)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch.geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch.geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch.geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch.geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch.geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch.geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch.geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch.geometric) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch.geometric) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch.geometric) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch.geometric) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch.geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch.geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch.geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch.geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch.geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch.geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch.geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch.geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch.geometric) (2024.12.14)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch.geometric) (4.12.2)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch.geometric\n",
            "Successfully installed torch.geometric-2.6.1\n",
            "Collecting rdkit\n",
            "  Downloading rdkit-2024.9.3-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (11.0.0)\n",
            "Downloading rdkit-2024.9.3-cp310-cp310-manylinux_2_28_x86_64.whl (33.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.8/33.8 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit\n",
            "Successfully installed rdkit-2024.9.3\n",
            "Collecting pubchempy\n",
            "  Downloading PubChemPy-1.0.4.tar.gz (29 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pubchempy\n",
            "  Building wheel for pubchempy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pubchempy: filename=PubChemPy-1.0.4-py3-none-any.whl size=13819 sha256=c07bf9f1d3f31b703106a936501ccb1102cc28c5d756051d6b0a66182a153a71\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/7c/45/18a0671e3c3316966ef7ed9ad2b3f3300a7e41d3421a44e799\n",
            "Successfully built pubchempy\n",
            "Installing collected packages: pubchempy\n",
            "Successfully installed pubchempy-1.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "7S3rsXs5I1XH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch_geometric.data import Data, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from rdkit import Chem\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#Read the CSV file\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/aadarsh0709/gnnmodelpfas/refs/heads/main/S3_Dawsonetal_PFAS_HL_101122.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "OJSWtHv4vEX9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "1egyjJgOI1XK"
      },
      "outputs": [],
      "source": [
        "#Get the atom features (Node)\n",
        "def atom_features(atom):\n",
        "    return torch.tensor([\n",
        "        atom.getMass(),\n",
        "        atom.GetTotalNumHs(),\n",
        "        atom.GetAtomicNum(),\n",
        "        atom.GetDegree(),\n",
        "        atom.GetFormalCharge(),\n",
        "        atom.GetNumRadicalElectrons(),\n",
        "        int(atom.GetChiralTag()),\n",
        "        int(atom.GetIsAromatic()),\n",
        "        int(atom.IsInRing()),\n",
        "    ], dtype=torch.float32)\n",
        "\n",
        "#Get the bond features (Edge)\n",
        "def bond_features(bond):\n",
        "    return torch.tensor([\n",
        "        int(bond.GetBondType()),\n",
        "        int(bond.GetIsConjugated()),\n",
        "        int(bond.IsInRing()),\n",
        "        int(bond.GetBondDir())\n",
        "    ], dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "Nc33kP67I1XL"
      },
      "outputs": [],
      "source": [
        "import pubchempy as pcp\n",
        "\n",
        "def get_smiles_by_cas(cas_number):\n",
        "    \"\"\"\n",
        "    Retrieves the SMILES string for a given CAS number using PubChem.\n",
        "\n",
        "    Parameters:\n",
        "        cas_number (str): The CAS number of the compound.\n",
        "\n",
        "    Returns:\n",
        "        str: The corresponding SMILES string if found, otherwise an error message.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Search for compounds using the CAS number\n",
        "        compounds = pcp.get_compounds(cas_number, 'name')\n",
        "        if not compounds:\n",
        "            return \"Not Found\"\n",
        "\n",
        "        # Retrieve the SMILES string of the first matched compound\n",
        "        compound = compounds[0]\n",
        "        return [ compound.exact_mass, compound.isomeric_smiles]\n",
        "    except Exception as e:\n",
        "        # Return the error message in case of an exception\n",
        "        return f\"Error: {e}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "6SFYyINdI1XL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "4bf1889c-a0ed-40de-edfc-471f2c161510"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of unique casrns= {} 6603\n"
          ]
        }
      ],
      "source": [
        "#pull list of unique casrns\n",
        "unique_casrns = list(df[\"CASRN\"].unique())\n",
        "smiles_dict = {}\n",
        "mass_dict = {}\n",
        "count=0\n",
        "print(\"# of unique casrns= {}\",len(unique_casrns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "gXqC_TmqI1XM",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b201be0b-832b-429b-b791-4b0b09785dea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "2000\n",
            "2100\n",
            "2200\n",
            "2300\n",
            "2400\n",
            "2500\n",
            "2600\n",
            "2700\n",
            "2800\n",
            "2900\n",
            "3000\n",
            "3100\n",
            "3200\n",
            "3300\n",
            "3400\n",
            "3500\n",
            "3600\n",
            "3700\n",
            "3800\n",
            "3900\n",
            "4000\n",
            "4100\n",
            "4200\n",
            "4300\n",
            "4400\n",
            "4500\n",
            "4600\n",
            "4700\n",
            "4800\n",
            "4900\n",
            "5000\n",
            "5100\n",
            "5200\n",
            "5300\n",
            "5400\n",
            "5500\n",
            "5600\n",
            "5700\n",
            "5800\n",
            "5900\n",
            "6000\n",
            "6100\n",
            "6200\n",
            "6300\n",
            "6400\n",
            "6500\n",
            "6600\n"
          ]
        }
      ],
      "source": [
        "# Iterate through unique CASRNs and get SMILES\n",
        "\n",
        "for casrn in unique_casrns:\n",
        "    smilesandmass =  get_smiles_by_cas(casrn)\n",
        "    smiles_dict[casrn] = smilesandmass[1]\n",
        "    mass_dict[casrn] =  smilesandmass[0]\n",
        "    if (count%100 ==0 ):\n",
        "        print ( count )\n",
        "    count +=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "bHoxifNbI1XM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b620d4a-97d5-487b-b245-8c6072846179"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique CASRNs: 6603\n",
            "CASRNs with missing SMILES:\n",
            "Empty DataFrame\n",
            "Columns: [CASRN]\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "#Add the SMILES column to the DataFrame\n",
        "df['SMILES'] = df['CASRN'].map(smiles_dict)\n",
        "df['MASS'] = df['CASRN'].map(mass_dict)\n",
        "\n",
        "#Remove the parts where the CASRN has no SMILES in the DataFrame\n",
        "df = df[(df['SMILES'].notna()) & (df['SMILES'] != 'Not Found')]\n",
        "\n",
        "unique_casrns_count = df['CASRN'].nunique()\n",
        "print(f\"Number of unique CASRNs: {unique_casrns_count}\")\n",
        "\n",
        "missing_smiles = df[df['SMILES'].isnull()]\n",
        "print(\"CASRNs with missing SMILES:\")\n",
        "print(missing_smiles[['CASRN']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "o5L-Bm5_I1XN"
      },
      "outputs": [],
      "source": [
        "#Convert the smiles information into graph format utilizing torch and setting the lists\n",
        "def smiles_to_graph(smiles_string):\n",
        "    \"\"\"\n",
        "    Converts a SMILES string into a PyTorch Geometric Data object representing a molecular graph.\n",
        "\n",
        "    Args:\n",
        "        smiles_string (str): The SMILES string representing a molecule.\n",
        "\n",
        "    Returns:\n",
        "        Data: A PyTorch Geometric Data object containing node features, edge indices, and edge features.\n",
        "    \"\"\"\n",
        "    if not smiles_string:  # Check if smiles is None or empty\n",
        "        return None  # Return None if SMILES is invalid\n",
        "\n",
        "    molecule = Chem.MolFromSmiles(smiles_string)\n",
        "    if molecule is None:  # Check if RDKit failed to create a molecule\n",
        "      return None  # Return None if conversion failed\n",
        "\n",
        "    # Node features (atom types)\n",
        "    num_atoms = molecule.GetNumAtoms()\n",
        "    node_features = []\n",
        "    for atom in molecule.GetAtoms():\n",
        "        node_features.append([atom.GetAtomicNum()])  # Example: atomic number as feature\n",
        "\n",
        "    # Edge features (bond types)\n",
        "    edges = []\n",
        "    edge_features = []\n",
        "    for bond in molecule.GetBonds():\n",
        "        start_node = bond.GetBeginAtomIdx()\n",
        "        end_node = bond.GetEndAtomIdx()\n",
        "        edges.append([start_node, end_node])\n",
        "        edges.append([end_node, start_node])  # For undirected graph\n",
        "        edge_features.append([bond.GetBondType()])\n",
        "        edge_features.append([bond.GetBondType()])\n",
        "\n",
        "    # Convert to tensors\n",
        "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
        "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "    edge_attr = torch.tensor(edge_features, dtype=torch.float)\n",
        "\n",
        "    graph_data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr)\n",
        "    return graph_data\n",
        "\n",
        "\n",
        "def smiles_to_graph1(smiles):\n",
        "\n",
        "    if not smiles:  # Check if smiles is None or empty\n",
        "        return None  # Return None if SMILES is invalid\n",
        "\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:  # Check if RDKit failed to create a molecule\n",
        "        return None  # Return None if conversion failed\n",
        "\n",
        "    atom_feats = torch.stack([atom_features(atom) for atom in mol.GetAtoms()])\n",
        "\n",
        "    edge_indices = []\n",
        "    edge_feats = []\n",
        "    for bond in mol.GetBonds():\n",
        "        i = bond.GetBeginAtomIdx()\n",
        "        j = bond.GetEndAtomIdx()\n",
        "        edge_indices += [[i, j], [j, i]]\n",
        "        edge_feats += [bond_features(bond)] * 2\n",
        "\n",
        "    edge_index = torch.tensor(edge_indices).t().contiguous()\n",
        "    edge_attr = torch.stack(edge_feats) if edge_feats else None\n",
        "\n",
        "    return Data(x=atom_feats, edge_index=edge_index, edge_attr=edge_attr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "J3ontAAAI1XO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "\n",
        "binsize=5\n",
        "\n",
        "# K-means binning\n",
        "kmeans_binner = KBinsDiscretizer(n_bins=binsize, encode='ordinal', strategy='kmeans', random_state =0)\n",
        "df['kmeans_bin'] = kmeans_binner.fit_transform(df[['HalfLife.h']])\n",
        "\n",
        "\n",
        "# Quantile-based binning\n",
        "quantile_binner = KBinsDiscretizer(n_bins=binsize, encode='ordinal', strategy='quantile', random_state =0)\n",
        "df['quantile_bin'] = quantile_binner.fit_transform(df[['HalfLife.h']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "5wPW6BhxI1XP"
      },
      "outputs": [],
      "source": [
        "import typing\n",
        "\n",
        "def one_hot_encode(items: list) -> typing.List[list]:\n",
        "    results = []\n",
        "    # find the unique items (we want to unique items b/c duplicate items will have the same encoding)\n",
        "    unique_items = list(set(items))\n",
        "    # sort the unique items\n",
        "    sorted_items = sorted(unique_items)\n",
        "    # find how long the list of each item should be\n",
        "    max_index = len(unique_items)\n",
        "\n",
        "    for item in items:\n",
        "        # create a list of zeros the appropriate length\n",
        "        one_hot_encoded_result = [0 for i in range(0, max_index)]\n",
        "        # find the index of the item\n",
        "        one_hot_index = sorted_items.index(item)\n",
        "        # change the zero at the index from the previous line to a one\n",
        "        one_hot_encoded_result[one_hot_index] = 1\n",
        "        # add the result\n",
        "        results.append(one_hot_encoded_result)\n",
        "\n",
        "    return results\n",
        "\n",
        "df[\"Species\"] = one_hot_encode(df[\"Species\"])\n",
        "df[\"Sex\"] = one_hot_encode(df[\"Sex\"])\n",
        "df[\"DosingAdj\"] = one_hot_encode(df[\"DosingAdj\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Grouping of Data based on species\n",
        "\n",
        "#Getting a list of all of the species\n",
        "df[\"string_spc\"] = df[\"Species\"].astype(str)\n",
        "species_list = df['string_spc'].unique()\n",
        "#Printing the species list\n",
        "print(species_list)\n",
        "#Group data based off species\n",
        "grouped_data = df.groupby('string_spc')\n",
        "#Iterate through the species split data\n",
        "\"\"\"\n",
        "for species, group in grouped_data:\n",
        "    print(f\"Species: {species}\")\n",
        "    print(group)\n",
        "    print(\"\\n\")\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "r4WdAlCiUyWg",
        "outputId": "937fb8cf-dbe8-4b2b-c58b-3c8690f7a113"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[1, 0, 0, 0, 0, 0, 0, 0, 0]' '[0, 1, 0, 0, 0, 0, 0, 0, 0]'\n",
            " '[0, 0, 1, 0, 0, 0, 0, 0, 0]' '[0, 0, 0, 1, 0, 0, 0, 0, 0]'\n",
            " '[0, 0, 0, 0, 1, 0, 0, 0, 0]' '[0, 0, 0, 0, 0, 1, 0, 0, 0]'\n",
            " '[0, 0, 0, 0, 0, 0, 1, 0, 0]' '[0, 0, 0, 0, 0, 0, 0, 1, 0]'\n",
            " '[0, 0, 0, 0, 0, 0, 0, 0, 1]']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor species, group in grouped_data:\\n    print(f\"Species: {species}\")\\n    print(group)\\n    print(\"\\n\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "MO6F2zV6I1XO",
        "outputId": "d2a2c9c2-456e-4f11-b209-f2c70171df4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data set count= 4849182\n",
            "Training data set count= 1212372\n"
          ]
        }
      ],
      "source": [
        "#For each of the species group split into train and test data\n",
        "\n",
        "#Set the lists for both train and test\n",
        "train_data = []\n",
        "test_data = []\n",
        "\n",
        "#For loop that will iterate through the species in order to split the data\n",
        "for Species,group in grouped_data:\n",
        "    train_group, test_group = train_test_split(group, test_size=0.2, random_state=42)\n",
        "    train_data.append(train_group)\n",
        "    test_data.append(test_group)\n",
        "\n",
        "#Combination of the split data\n",
        "train_set = pd.concat(train_data)\n",
        "test_set = pd.concat(test_data)\n",
        "\n",
        "\n",
        "#Test if training and testing set properly function\n",
        "print(\"Training data set count=\",train_set.size)\n",
        "print(\"Training data set count=\",test_set.size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "lGuBbyFwI1XP"
      },
      "outputs": [],
      "source": [
        "train_set.to_csv(\"train.csv\")\n",
        "test_set.to_csv(\"test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_p_WoVfI1XP",
        "outputId": "45778a60-0c5e-46b3-e9ab-7a1cf3ec93d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique CASRNs: 6603\n",
            "CASRNs with missing SMILES:\n",
            "Empty DataFrame\n",
            "Columns: [CASRN]\n",
            "Index: []\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "6LZDdxdSvnTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mw_7FDUC_PSv",
        "outputId": "ab2d8581-5c41-493e-9dc5-4198d1163f4f"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.plotting import table\n",
        "from rdkit.Chem import Draw\n",
        "from rdkit import Chem\n",
        "from sklearn.metrics import r2_score\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch_geometric.datasets import MoleculeNet\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
        "from torch_geometric.utils import to_networkx\n",
        "from torch.nn import Linear\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pubchempy\n",
        "import rdkit\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "1JsJo46Wg5hs"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch_geometric.data import Dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "class MoleculeDatasetMemory(Dataset):\n",
        "    def __init__(self, root, dataframe, bin_type, transform=None, pre_transform=None, pre_filter=None):\n",
        "        self.dataframe = dataframe.reset_index(drop=True)\n",
        "        self.bin_type = bin_type\n",
        "        self.data_list = []  # Store all processed data in memory\n",
        "        super().__init__(root, transform, pre_transform, pre_filter)\n",
        "\n",
        "        self._load_processed_data()\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return []  # We don't have raw files as we're using a dataframe\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return ['memory_dataset.pt']  # We'll save the entire dataset as one file\n",
        "\n",
        "    def download(self):\n",
        "        pass  # We don't need to download anything\n",
        "\n",
        "    def process(self):\n",
        "        for idx, row in tqdm(self.dataframe.iterrows(), total=self.dataframe.shape[0]):\n",
        "            smiles = row['SMILES']\n",
        "            graph = smiles_to_graph(smiles)\n",
        "            if graph is not None:  # Only process valid graphs\n",
        "                c_name = self._get_column_name()\n",
        "                graph.y = torch.tensor([float(row[c_name])], dtype=torch.float32)\n",
        "                graph.species = torch.tensor(row['Species'], dtype=torch.float)\n",
        "                graph.sex = torch.tensor(row['Sex'], dtype=torch.float)\n",
        "                graph.dosing_adj = torch.tensor(row['DosingAdj'], dtype=torch.float)\n",
        "                self.data_list.append(graph)\n",
        "\n",
        "        # Save the entire dataset to a single file\n",
        "        torch.save(self.data_list, os.path.join(self.processed_dir, 'memory_dataset.pt'))\n",
        "\n",
        "    def len(self):\n",
        "        return len(self.data_list)\n",
        "\n",
        "    def _load_processed_data(self):\n",
        "        if os.path.exists(self.processed_paths[0]):\n",
        "            print(\"Loading preprocessed dataset from file.\")\n",
        "            self.data_list = torch.load(self.processed_paths[0])\n",
        "        else:\n",
        "            print(\"Preprocessed dataset not found. It will be created when accessing data.\")\n",
        "\n",
        "    def get(self, idx):\n",
        "        return self.data_list[idx]\n",
        "\n",
        "    def _get_column_name(self):\n",
        "        if self.bin_type == 'raw':\n",
        "            return 'HalfLife.h'\n",
        "        elif self.bin_type in ['kmeans_bin', 'equal_width_bin', 'quantile_bin', \"half_life_binned\"]:\n",
        "            return self.bin_type\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown bin_type: {self.bin_type}\")"
      ],
      "metadata": {
        "id": "AlEtqXYrvfTv"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "H2WHuTH0I1XP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b53c9c2c-2d93-44b4-93ca-db14b215c410"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading preprocessed dataset from file.\n",
            "Loading preprocessed dataset from file.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "train_set = pd.read_csv(\"/content/drive/MyDrive/train.csv\")\n",
        "test_set = pd.read_csv(\"/content/drive/MyDrive/test.csv\")\n",
        "\n",
        "train_dataset2 = MoleculeDatasetMemory(\"/content/drive/MyDrive/traindatafinal_newmemory\", train_set, 'kmeans_bin')\n",
        "test_dataset2 = MoleculeDatasetMemory(\"/content/drive/MyDrive/testdatafinal_newmemory\", test_set, 'kmeans_bin')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "RgRkrnfWI1XP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd22308e-c70e-4f3c-d4e4-e9dea557d9f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN(\n",
            "  (conv1): GCNConv(3, 5)\n",
            "  (conv2): GCNConv(5, 5)\n",
            "  (conv3): GCNConv(5, 5)\n",
            "  (species_lin): Linear(in_features=9, out_features=5, bias=True)\n",
            "  (sex_lin): Linear(in_features=2, out_features=5, bias=True)\n",
            "  (dosing_adj_lin): Linear(in_features=3, out_features=5, bias=True)\n",
            "  (lin): Linear(in_features=20, out_features=10, bias=True)\n",
            ")\n",
            "Number of parameters:  375\n"
          ]
        }
      ],
      "source": [
        "from re import X\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import NNConv, global_mean_pool\n",
        "from torch.nn import Sequential, Linear, ReLU\n",
        "import ast\n",
        "\n",
        "# Calculate accuracy r2\n",
        "def r2_accuracy(pred_y, y):\n",
        "    score = r2_score(y, pred_y)\n",
        "    return round(score, 2)*100\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, num_node_features, hidden_channels, num_classes=10):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "\n",
        "        # Linear layers for graph-level features\n",
        "        self.species_lin = torch.nn.Linear(9, hidden_channels)\n",
        "        self.sex_lin = torch.nn.Linear(2, hidden_channels)\n",
        "        self.dosing_adj_lin = torch.nn.Linear(3, hidden_channels)\n",
        "\n",
        "        self.lin = torch.nn.Linear(hidden_channels * 4, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch, species, sex, dosing_adj):\n",
        "\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "        species_emb = F.relu(self.species_lin(species.view(-1, 9)))\n",
        "        sex_emb = F.relu(self.sex_lin(sex.view(-1, 2)))\n",
        "        dosing_adj_emb = F.relu(self.dosing_adj_lin(dosing_adj.view(-1, 3)))\n",
        "\n",
        "        x = torch.cat([x, species_emb, sex_emb, dosing_adj_emb], dim=1)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "        # Classifier (Linear).\n",
        "        x = self.lin(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "def train(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for i, data in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        out = model(data.x, data.edge_index, data.batch,\n",
        "                    data.species, data.sex, data.dosing_adj)\n",
        "\n",
        "        #new code\n",
        "        #loss = loss_fn(pred, data.y)\n",
        "        loss = criterion(out, data.y.long())\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct = (pred == data.y).sum().item()\n",
        "        total = data.y.size(0)\n",
        "        acc = (correct / total)*100\n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return loss, acc, pred, data.y, out\n",
        "\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            out = model(data.x, data.edge_index, data.batch,\n",
        "                    data.species, data.sex, data.dosing_adj)\n",
        "            pred = out.argmax(dim=1)\n",
        "            correct += (pred == data.y).sum().item()\n",
        "            total += data.y.size(0)\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "model = GCN(3,5)\n",
        "print(model)\n",
        "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import DataLoader\n",
        "from sklearn.metrics import r2_score\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import time\n",
        "\n",
        "# Root mean squared error\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0007)\n",
        "\n",
        "\n",
        "# Data generated\n",
        "embeddings = []\n",
        "losses = []\n",
        "accuracies = []\n",
        "outputs = []\n",
        "targets = []\n",
        "\n",
        "# Model initialization\n",
        "num_node_features = train_dataset2.get(1).num_node_features\n",
        "num_graph_features = 5\n",
        "num_edge_features = train_dataset2.get(1).num_edge_features\n",
        "hidden_channels = 64\n",
        "num_classes = 10\n",
        "\n",
        "model = GCN(num_node_features, hidden_channels, num_classes)\n",
        "\n",
        "print(model)\n",
        "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))\n",
        "\n",
        "# Optimizer and loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "# Use GPU for training, if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Data loader\n",
        "NUM_GRAPHS_PER_BATCH = 256\n",
        "NUM_EPOCHS = 250\n",
        "\n",
        "torch.manual_seed(12345)\n",
        "\n",
        "train_loader2 = DataLoader(train_dataset2, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True, num_workers = 32, pin_memory=True)\n",
        "test_loader2 = DataLoader(test_dataset2, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True, num_workers = 32, pin_memory=True)\n",
        "\n",
        "\n",
        "print('\\n======== data distribution =======\\n')\n",
        "print(\"Size of training data: {} graphs\".format(len(train_dataset2)))\n",
        "print(\"Size of testing data: {} graphs\".format(len(test_dataset2)))\n",
        "\n",
        "\n",
        "print('\\n======== Starting training ... =======\\n')\n",
        "start_time = time.time()\n",
        "\n",
        "losses = []\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    loss, acc, pred, target, h = train(model, train_loader2, optimizer, criterion, device)\n",
        "    losses.append(loss)\n",
        "    accuracies.append(acc)\n",
        "    outputs.append(pred)\n",
        "    targets.append(target)\n",
        "\n",
        "    if epoch % 25 == 0:\n",
        "      # print(f\"Epoch {epoch} | Train Loss {loss}\")\n",
        "      print(f'Epoch {epoch:>3} | Loss: {loss:.5f} | Acc: {acc:.2f}%')\n",
        "\n",
        "print(\"\\nTraining done!\\n\")\n",
        "elapsed = time.time() - start_time\n",
        "minutes_e = elapsed//60\n",
        "print(\"--- training took:  %s minutes ---\" % (minutes_e))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbwJvZE-IR7Z",
        "outputId": "01e664ca-bb70-4e5d-f7e2-244b145746dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN(\n",
            "  (conv1): GCNConv(7, 64)\n",
            "  (conv2): GCNConv(64, 64)\n",
            "  (conv3): GCNConv(64, 64)\n",
            "  (species_lin): Linear(in_features=9, out_features=64, bias=True)\n",
            "  (sex_lin): Linear(in_features=2, out_features=64, bias=True)\n",
            "  (dosing_adj_lin): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (lin): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n",
            "Number of parameters:  12490\n",
            "\n",
            "======== data distribution =======\n",
            "\n",
            "Size of training data: 279585 graphs\n",
            "Size of testing data: 69903 graphs\n",
            "\n",
            "======== Starting training ... =======\n",
            "\n",
            "Epoch   0 | Loss: 0.58188 | Acc: 72.73%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdmU_vGBI1XQ"
      },
      "outputs": [],
      "source": [
        "# List of different batch sizes to iterate through\n",
        "\n",
        "\n",
        "train_loader2 = DataLoader(train_dataset2, batch_size=256, shuffle=True, num_workers = 32, pin_memory=True)\n",
        "test_loader2 = DataLoader(test_dataset2, batch_size=256, shuffle=True, num_workers = 32, pin_memory=True)\n",
        "\n",
        "\n",
        "# Model initialization\n",
        "num_node_features = train_dataset2.get(1).num_node_features\n",
        "num_graph_features = 5\n",
        "num_edge_features = train_dataset2.get(1).num_edge_features\n",
        "hidden_channels = 64\n",
        "num_classes = 10\n",
        "\n",
        "model = GCN(num_node_features, hidden_channels, num_classes)\n",
        "model.to(\"cuda\")\n",
        "\n",
        "print(model)\n",
        "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))\n",
        "\n",
        "# Optimizer and loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# Lists to store loss and accuracy\n",
        "train_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_loss = train(model=model, train_loader=train_loader2, optimizer=optimizer, criterion=criterion)\n",
        "    test_acc = evaluate(model, test_loader2)\n",
        "\n",
        "    # Store loss and accuracy\n",
        "    train_losses.append(train_loss)\n",
        "    test_accuracies.append(test_acc)\n",
        "\n",
        "    print(f'Epoch {epoch:03d}, Loss: {train_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "# After training, train_losses and test_accuracies will contain the collected information\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "collapsed_sections": [
        "OJSWtHv4vEX9"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}